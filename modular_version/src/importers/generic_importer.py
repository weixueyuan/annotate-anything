#!/usr/bin/env python
"""
é€šç”¨æ•°æ®å¯¼å…¥å™¨

æ”¯æŒå¯¼å…¥ä»»ä½•æ ‡å‡† JSONL æ ¼å¼çš„æ•°æ®åˆ°æ•°æ®åº“
é€‚ç”¨äºæ‰€æœ‰ä»»åŠ¡ï¼šannotation, whole_annotation, part_annotation ç­‰

ä½¿ç”¨æ–¹å¼ï¼š
    # å¯¼å…¥æ‰€æœ‰ä»»åŠ¡ï¼ˆé»˜è®¤æ¸…ç©ºï¼‰
    python -m importers.generic_importer
    
    # å¢é‡å¯¼å…¥æ‰€æœ‰ä»»åŠ¡
    python -m importers.generic_importer --incremental
    
    # æŒ‰ä»»åŠ¡åå¯¼å…¥ï¼ˆé»˜è®¤æ¸…ç©ºï¼‰
    python -m importers.generic_importer --task whole_annotation
    
    # æŒ‰ä»»åŠ¡åå¢é‡å¯¼å…¥
    python -m importers.generic_importer --task annotation --incremental
    
    # è‡ªå®šä¹‰è·¯å¾„
    python -m importers.generic_importer --source data.jsonl --db databases/custom.db
"""

import json
import os
import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
# generic_importer.py -> importers/ -> src/ -> modular_version/
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.db_models import Annotation, get_session, get_engine, Base


# ä»»åŠ¡é…ç½®æ˜ å°„ï¼ˆé»˜è®¤è·¯å¾„ï¼‰
TASK_CONFIGS = {
    'annotation': {
        'source': 'database_jsonl/merged_attributes.jsonl',
        'db': 'databases/annotation.db',
        'description': 'ç‰©ä½“å±æ€§æ ‡æ³¨'
    },
    'whole_annotation': {
        'source': 'database_jsonl/whole_annotation.jsonl',
        'db': 'databases/whole_annotation.db',
        'description': 'æ•´ä½“ç‰©ä½“æ ‡æ³¨'
    },
    'part_annotation': {
        'source': 'database_jsonl/part_annotation.jsonl',
        'db': 'databases/part_annotation.db',
        'description': 'éƒ¨ä»¶æ ‡æ³¨'
    },
    'test': {
        'source': 'database_jsonl/test.jsonl',
        'db': 'databases/test.db',
        'description': 'æµ‹è¯•æ•°æ®'
    }
}


class GenericImporter:
    """é€šç”¨æ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self):
        self.stats = {'imported': 0, 'updated': 0, 'errors': 0}
    
    def parse_jsonl(self, filepath: str):
        """è§£æJSONLæ–‡ä»¶"""
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        
        records = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        data = json.loads(line)
                        records.append(data)
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸  ç¬¬ {line_num} è¡Œ JSON è§£æé”™è¯¯: {e}")
                        continue
        return records
    
    def transform_record(self, model_id: str, attrs: dict) -> tuple:
        """
        è½¬æ¢å•æ¡è®°å½• - é€šç”¨å¤„ç†
        
        è‡ªåŠ¨è¯†åˆ«å’Œå¤„ç†ï¼š
        - å…ƒæ•°æ®å­—æ®µï¼ˆannotated, uid, scoreï¼‰
        - æ•°ç»„å­—æ®µï¼ˆè‡ªåŠ¨è½¬ä¸ºæ¢è¡Œç¬¦åˆ†éš”çš„å­—ç¬¦ä¸²ï¼‰
        - å…¶ä»–å­—æ®µä¿æŒåŸæ ·
        """
        # å…ƒæ•°æ®ï¼ˆä»attrsä¸­æå–ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ç”¨é»˜è®¤å€¼ï¼‰
        metadata = {
            'annotated': attrs.get('annotated', False),
            'uid': attrs.get('uid', ''),
            'score': attrs.get('score', 1),
        }
        
        # ä¸šåŠ¡æ•°æ® - é€šç”¨å¤„ç†
        business_data = {}
        for key, value in attrs.items():
            # è·³è¿‡å…ƒæ•°æ®å­—æ®µ
            if key in ['annotated', 'uid', 'score']:
                continue
            
            # è‡ªåŠ¨å¤„ç†æ•°ç»„å­—æ®µï¼šè½¬ä¸ºå­—ç¬¦ä¸²
            if isinstance(value, list):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²æ•°ç»„ï¼Œç”¨æ¢è¡Œç¬¦è¿æ¥
                if value and isinstance(value[0], str):
                    business_data[key] = '\n'.join(value)
                # å¦‚æœæ˜¯å…¶ä»–ç±»å‹çš„æ•°ç»„ï¼Œç”¨é€—å·è¿æ¥
                else:
                    business_data[key] = ', '.join(str(v) for v in value)
            else:
                # å…¶ä»–å­—æ®µä¿æŒåŸæ ·
                business_data[key] = value
        
        return metadata, business_data
    
    def import_to_db(self, source: str, db_path: str, clean: bool = False, batch_size: int = 1000):
        """å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹å¯¼å…¥æ•°æ®")
        print(f"{'='*60}")
        print(f"ğŸ“‚ æ•°æ®æº: {source}")
        print(f"ğŸ—„ï¸  æ•°æ®åº“: {db_path}")
        
        # åˆå§‹åŒ–æ•°æ®åº“
        engine = get_engine(db_path)
        if clean:
            print("ğŸ—‘ï¸  æ¸…ç©ºæ•°æ®åº“...")
            Base.metadata.drop_all(engine)
        Base.metadata.create_all(engine)
        
        session = get_session(db_path)
        
        try:
            print(f"ğŸ“– è§£ææ•°æ®...")
            records = self.parse_jsonl(source)
            print(f"âœ“ æ‰¾åˆ° {len(records)} æ¡è®°å½•")
            
            for idx, record in enumerate(records, 1):
                try:
                    # è·å– model_id å’Œå±æ€§
                    if not record:
                        continue
                    
                    model_id = list(record.keys())[0]
                    attrs = record[model_id]
                    
                    # è½¬æ¢æ•°æ®
                    metadata, business_data = self.transform_record(model_id, attrs)
                    
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    existing = session.query(Annotation).filter_by(model_id=model_id).first()
                    
                    if existing:
                        # æ›´æ–°
                        existing.annotated = metadata['annotated']
                        existing.uid = metadata['uid']
                        existing.score = metadata['score']
                        existing.data = business_data
                        self.stats['updated'] += 1
                    else:
                        # æ–°å¢
                        annotation = Annotation(
                            model_id=model_id,
                            annotated=metadata['annotated'],
                            uid=metadata['uid'],
                            score=metadata['score'],
                            data=business_data
                        )
                        session.add(annotation)
                        self.stats['imported'] += 1
                    
                    # æ‰¹é‡æäº¤
                    if idx % batch_size == 0:
                        session.commit()
                        print(f"  å·²å¤„ç† {idx} æ¡...")
                
                except Exception as e:
                    self.stats['errors'] += 1
                    if self.stats['errors'] <= 5:
                        print(f"âš ï¸  ç¬¬ {idx} æ¡é”™è¯¯: {e}")
            
            session.commit()
            
            # æ‰“å°ç»Ÿè®¡
            print(f"\n{'='*60}")
            print(f"âœ… å¯¼å…¥å®Œæˆï¼")
            print(f"{'='*60}")
            print(f"ğŸ“Š ç»Ÿè®¡:")
            print(f"  - æ–°å¢: {self.stats['imported']} æ¡")
            print(f"  - æ›´æ–°: {self.stats['updated']} æ¡")
            print(f"  - é”™è¯¯: {self.stats['errors']} æ¡")
            print(f"{'='*60}\n")
            
        except Exception as e:
            session.rollback()
            print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
            raise
        finally:
            session.close()


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description='é€šç”¨æ•°æ®å¯¼å…¥å™¨ - æ”¯æŒæ‰€æœ‰æ ‡å‡† JSONL æ ¼å¼',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¯¼å…¥æ‰€æœ‰ä»»åŠ¡ï¼ˆé»˜è®¤æ¸…ç©ºï¼‰
  python -m importers.generic_importer

  # å¢é‡å¯¼å…¥æ‰€æœ‰ä»»åŠ¡
  python -m importers.generic_importer --incremental
  
  # æŒ‰ä»»åŠ¡åå¯¼å…¥ï¼ˆé»˜è®¤æ¸…ç©ºï¼‰
  python -m importers.generic_importer --task annotation
  
  # æŒ‰ä»»åŠ¡åå¢é‡å¯¼å…¥
  python -m importers.generic_importer --task annotation --incremental
  
  # è‡ªå®šä¹‰è·¯å¾„
  python -m importers.generic_importer --source data.jsonl --db databases/custom.db

æ”¯æŒçš„ä»»åŠ¡:
"""
    )
    
    # æ·»åŠ ä»»åŠ¡åˆ—è¡¨åˆ°å¸®åŠ©ä¿¡æ¯
    for task_name, config in TASK_CONFIGS.items():
        parser.epilog += f"  {task_name:20s} - {config['description']}\n"
    
    parser.add_argument('--task', '-t', type=str, choices=list(TASK_CONFIGS.keys()),
                       help='ä»»åŠ¡åç§°ï¼ˆè‡ªåŠ¨ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰')
    parser.add_argument('--source', '-s', type=str,
                       help='æ•°æ®æºæ–‡ä»¶ï¼ˆJSONLæ ¼å¼ï¼‰')
    parser.add_argument('--db', '-d', type=str,
                       help='æ•°æ®åº“è·¯å¾„')
    parser.add_argument('--incremental', '-i', action='store_true',
                       help='å¢é‡å¯¼å…¥ï¼ˆä¸æ¸…é™¤æ—§æ•°æ®ï¼‰ï¼Œé»˜è®¤ä¸ºæ¸…ç©ºå¯¼å…¥')
    parser.add_argument('--list', '-l', action='store_true',
                       help='åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„ä»»åŠ¡')
    
    args = parser.parse_args()
    
    # åˆ—å‡ºä»»åŠ¡
    if args.list:
        print("\nğŸ“‹ æ”¯æŒçš„ä»»åŠ¡:")
        print("=" * 70)
        for task_name, config in TASK_CONFIGS.items():
            print(f"\nä»»åŠ¡åç§°: {task_name}")
            print(f"  æè¿°: {config['description']}")
            print(f"  æ•°æ®æº: {config['source']}")
            print(f"  æ•°æ®åº“: {config['db']}")
        print("\n" + "=" * 70)
        print("\nä½¿ç”¨æ–¹å¼: python -m importers.generic_importer --task <ä»»åŠ¡å>\n")
        return
    
    # ç¡®å®šå¯¼å…¥æ¨¡å¼
    clean_mode = not args.incremental
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•å‚æ•°ï¼Œåˆ™å¯¼å…¥æ‰€æœ‰ä»»åŠ¡
    if not args.task and not args.source and not args.db:
        print(f"\nğŸš€ é»˜è®¤æ‰§è¡Œï¼šå¯¼å…¥æ‰€æœ‰ä»»åŠ¡ ({'æ¸…ç©ºæ¨¡å¼' if clean_mode else 'å¢é‡æ¨¡å¼'})")
        importer = GenericImporter()
        for task_name, config in TASK_CONFIGS.items():
            importer.stats = {'imported': 0, 'updated': 0, 'errors': 0} # é‡ç½®ç»Ÿè®¡
            print(f"\n---\nğŸ”„ æ­£åœ¨å¤„ç†ä»»åŠ¡: {task_name}...")
            source = os.path.join(project_root, config['source'])
            db_path = os.path.join(project_root, config['db'])
            
            if not os.path.exists(source):
                print(f"âš ï¸  è­¦å‘Š: æ•°æ®æºä¸å­˜åœ¨ï¼Œè·³è¿‡: {source}")
                continue
            
            os.makedirs(os.path.dirname(db_path), exist_ok=True)
            importer.import_to_db(source=source, db_path=db_path, clean=clean_mode)
        
        print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å¯¼å…¥å®Œæˆï¼\n")
        return

    # å¤„ç†å•ä¸ªä»»åŠ¡æˆ–è‡ªå®šä¹‰è·¯å¾„
    source, db_path = None, None
    if args.task:
        if args.source or args.db:
            parser.error("ä½¿ç”¨ --task æ—¶ï¼Œä¸åº”å†æŒ‡å®š --source æˆ– --db")
        task_config = TASK_CONFIGS[args.task]
        source = os.path.join(project_root, task_config['source'])
        db_path = os.path.join(project_root, task_config['db'])
        print(f"\nğŸ“ ä½¿ç”¨ä»»åŠ¡é…ç½®: {args.task} - {task_config['description']}")
    
    elif args.source and args.db:
        source = args.source
        db_path = args.db
    
    else:
        parser.error("è¯·æŒ‡å®š --taskï¼Œæˆ–åŒæ—¶æŒ‡å®š --source å’Œ --dbï¼Œæˆ–ä¸å¸¦å‚æ•°è¿è¡Œä»¥å¯¼å…¥æ‰€æœ‰ä»»åŠ¡")

    # æ£€æŸ¥å’Œæ‰§è¡Œ
    if not os.path.exists(source):
        print(f"\nâŒ é”™è¯¯: æ•°æ®æºæ–‡ä»¶ä¸å­˜åœ¨: {source}")
        return
    
    os.makedirs(os.path.dirname(db_path), exist_ok=True)
    
    importer = GenericImporter()
    importer.import_to_db(source=source, db_path=db_path, clean=clean_mode)
    
    if args.task:
        print(f"âœ… å¯ä»¥è¿è¡Œ: python src/main_multi.py --task {args.task} --dev --uid user1\n")
    else:
        print(f"âœ… å¯¼å…¥å®Œæˆï¼\n")


if __name__ == "__main__":
    main()

